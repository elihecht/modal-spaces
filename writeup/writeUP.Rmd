---
title: 'Materials for Modal Spaces'
author: "Jonathan Phillips & Eli Hecht"
date: "December 2022"
output:
  html_document:
    df_print: paged
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
library(tidyr)
library(dplyr)
library(tibble)
library(readr)
library(lme4)
library(knitr)
library(ggrepel)
library(irr)
library(purrr)

knitr::opts_chunk$set(echo = FALSE,dpi=300,fig.width=7)
```

## Study 1: Possibility Generation

### Participants

```{r participantspg,echo=FALSE}
# read in Study 1a-c data
pg1 <- read.csv("../data/pg1.csv")
pg2 <- read.csv("../data/pg2.csv")
pg3 <- read.csv("../data/pg3.csv")

# add id column
pg1 <- pg1 %>% rownames_to_column(var = "id") %>% 
  mutate(id = as.numeric(id))
pg2 <- pg2 %>% rownames_to_column(var = "id") %>%
  mutate(id = nrow(pg1) + as.numeric(id))
pg3 <- pg3 %>% rownames_to_column(var = "id") %>% 
  mutate(id = nrow(pg1) + nrow(pg2) + as.numeric(id))

# join data from three studies into one table
pgW <- bind_rows(pg1, pg2, pg3) %>% 
  filter(Progress == 100)

## issue with 25 being entered twice for one participant's age
pgW$age[pgW$age==2525] <- 25

# List of participants who gave non-sensiscal answers
exclude_ids <- c(57, 88, 91, 135, 151, 153, 158, 189, 248, 229)
```

Across three studies ($N_{1}$ = `r length(unique(pg1$id))`, $N_{2}$ = `r length(unique(pg2$id))`, $N_{3}$ = `r length(unique(pg3$id))`) we collected a total sample of `r length(unique(pgW$id))` participants ($M_{age}$ = `r round(mean(pgW$age,na.rm=T),2)`; $SD_{age}$ = `r round(sd(pgW$age,na.rm=T),2)`; `r table(pgW$gender)[[2]]` females) from Amazon Mechanical Turk ([www.mturk.com](www.mturk.com)).

### Procedure

In each study, participants were presented with six stories, what we're referring to as 'decision-making contexts', in random order, each describing an agent having to make a decision. All 18 contexts are presented in the 'text' column of [Table 1](#table-1).

For each story, participants were asked:

|     In this situation, what are some things you believe [Agent] could do? Please list 5
| \ 
After listing their responses for each scenario, participants were then asked to rate each of their answers on three 100 point
scales.

|     Consider one of the things you believed [Agent] could do: [Participant response].

|     *Probability Question:* How likely is it that [Agent] will do that thing?

|     *Morality Question:* How morally acceptable is it that [Agent] will do that thing?

|     *Normality Question:* How normal is it that [Agent] will do that thing?

### Results

```{r tidypg,echo=FALSE, warning=FALSE, message=FALSE}

# converts the data from wide, with one row per participant, to long, with one row for every response
pgL <- pgW %>% select(id, S1A1_4:S18A5_6, -c(demo:ses, contains("FL"), contains("."))) %>% 
  pivot_longer(-id, names_to = "question", values_to = "response",
               values_drop_na = T) %>% 
  separate(question, into = c("context", "answer"), sep = "A") %>% 
  separate(answer, into = c("answer", "question"), sep = "_") %>% 
  mutate(across(2:4, parse_number))  %>% 
  mutate(question = if_else(question %in% c(4, 7), "Probability",
                        if_else(question %in% c(5, 8), "Morality", "Normality")))

# removes participants who clearly didn't take survey seriously/put in nonsensical answers
pgL <- pgL %>% filter(!id %in% exclude_ids)

# remove one set of responses from one participant for one context, for which they put "running out of time" for all 5 responses. Otherwise participant had normal behavior
pgL <- pgL %>% filter(!(id == 141 & context == 9))

# removes answers for any scenario sets that don't have full responses (3 ratings for each of 5 questions)
pgL <- pgL %>% group_by(id, context) %>%
    filter(n()==15)

## creates a score for the average of the three judgments
pgL <- pgL %>% pivot_wider(names_from = question, values_from = response ) %>%
                mutate(avg=(Normality+Probability+Morality)/3) %>%
                pivot_longer(cols=c(Normality,Morality,Probability,avg),
                             names_to = "question",values_to ="response")

# percentage of time that a participant rates each answer for a given scenario highest. ties go to the first answer
perBest <- pgL %>% filter(question=="avg") %>%
  group_by(id, context) %>% 
  mutate(rank = rank(-response, ties.method = "first")) %>% 
  group_by(answer) %>% 
  summarise(numBest = sum(rank == 1), .groups = "drop") %>% 
  ungroup() %>% 
  mutate(perBest = numBest/sum(numBest)) 


```

```{r pgpredict, echo=FALSE, warning=FALSE, message=FALSE}
# spreads out data into 1 row per participant response with 3 ratings in separate columns, as well as the average
pgPredict <- pgL %>% pivot_wider(names_from = "question", values_from = "response")

# Here we predict the order in which an answer was generated based on how highly the answer is rated
if(file.exists("../computationOutputs/pgPredictP.rda")){
  # saves computational time to just read these tables instead of recalculating them every time
  pgPredictN <- readRDS("../computationOutputs/pgPredictN.rda")
  pgPredictP <- readRDS("../computationOutputs/pgPredictP.rda")
  pgPredictM <- readRDS("../computationOutputs/pgPredictM.rda")
} else{
  lmerSamp_Full <- lmer(as.numeric(answer) ~ 
                          scale(Morality) + scale(Normality) +  scale(Probability) +
                          (1|context) + 
                          (scale(Morality) + scale(Normality) +  scale(Probability) |id),
                        data=pgPredict)
  
  # lesion out probability
  lmerSamp_P <- lmer(as.numeric(answer) ~ 
                       scale(Morality) + scale(Normality) + 
                       (1|context) + 
                       (scale(Morality) + scale(Normality) +  scale(Probability) |id),
                     data=pgPredict)
  # lesion out morality
  lmerSamp_M <- lmer(as.numeric(answer) ~ 
                       scale(Probability) + scale(Normality) + 
                       (1|context) + 
                       (scale(Morality) + scale(Normality) +  scale(Probability) |id),
                     data=pgPredict)
  # lesion out normality
    lmerSamp_N <- lmer(as.numeric(answer) ~ 
                         scale(Morality) + scale(Probability) + 
                         (1|context) + 
                         (scale(Morality) + scale(Normality) +  scale(Probability) |id),
                       data=pgPredict)
  
  pgPredictP <- anova(lmerSamp_Full,lmerSamp_P)
  saveRDS(pgPredictP, file = "../computationOutputs/pgPredictP.rda")
  pgPredictM <- anova(lmerSamp_Full,lmerSamp_M)
  saveRDS(pgPredictM, file = "../computationOutputs/pgPredictM.rda")
  pgPredictN <- anova(lmerSamp_Full,lmerSamp_N)
    saveRDS(pgPredictN, file = "../computationOutputs/pgPredictN.rda")
  
}
#  anova predicting answer number by average rating of an answer
pgPredictAnova <- summary(aov(lm(as.numeric(answer) ~ avg, data=pgPredict)))


# Here we investigate the relationship between normality ratings and probability and morality ratings:
if(file.exists("../computationOutputs/pgNormInt.rda")){
  # saves computational time to just read these tables instead of recalculating them every time
  pgNormInt <- readRDS("../computationOutputs/pgNormInt.rda")
  pgNormP <- readRDS("../computationOutputs/pgNormP.rda")
  pgNormM <- readRDS("../computationOutputs/pgNormM.rda")
} else{
  # predicting normality from interaction of morality and probability
  lmerNorm_Full <- lmer(scale(Normality) ~ scale(Morality) * scale(Probability) + (1|context) +
                   (scale(Probability) * scale(Morality) |id), data=pgPredict)
  # predicting normality with morality and probability as separate fixed effects
  lmerNorm_Main <- lmer(scale(Normality) ~ scale(Morality) + scale(Probability) + (1|context) +
                   (scale(Probability) * scale(Morality) |id), data=pgPredict)
  # model with probability removed
  lmerNorm_P <- lmer(scale(Normality) ~ scale(Morality) + (1|context) +
                   (scale(Probability) * scale(Morality) |id), data=pgPredict)
  # model with morality removed
  lmerNorm_M <- lmer(scale(Normality) ~ scale(Probability) + (1|context) +
                   (scale(Probability) * scale(Morality) |id), data=pgPredict)
  

  pgNormInt <- anova(lmerNorm_Full,lmerNorm_Main) # effect of interaction
    saveRDS(pgNormInt, file = "../computationOutputs/pgNormInt.rda") # these just save the models so you don't have to rerun later
  pgNormP <- anova(lmerNorm_Main,lmerNorm_P) # effect of probability
    saveRDS(pgNormP, file = "../computationOutputs/pgNormP.rda")
  pgNormM <- anova(lmerNorm_Main,lmerNorm_M) # effect of morality
    saveRDS(pgNormM, file = "../computationOutputs/pgNormM.rda")
  
}
```

`r length(exclude_ids)` participants gave nonsensical answers and their responses were excluded from further analysis.

Participants' first responses for each scenario tended to be the one they rated highest, or tied for highest (`r round(perBest[[1,3]] * 100, 1)`% of the time).

The better an option is, the earlier it tended to be generated by a participant ($F$(`r pgPredictAnova[[1]][["Df"]][1]`$) =$ `r round(pgPredictAnova[[1]][["F value"]][1],2)`, $p <$ `r max(.001, round(pgPredictAnova[[1]][["Pr(>F)"]][1],3))`). Further, independent of normality and morality, the more probable a response was, the more likely it was to be generated earlier ($\chi^2$(`r pgPredictP$Df[[2]]`) = `r pgPredictP$Chisq[[2]] %>% round(2) %>% format(scientific=F)`, $p <$ `r max(.001, round(pgPredictP$"Pr(>Chisq)"[[2]], 3))`). The same is true for morality, where independent of the other ratings, the more moral a possibility was, the more likely it was to be generated earlier ($\chi^2$(`r pgPredictM$Df[[2]]`) = `r round(pgPredictM$Chisq[[2]], 2) %>% format(scientific=F)`, $p <$ `r max(.001, round(pgPredictM$"Pr(>Chisq)"[[2]], 3))`). Normality was not found to have a similar significant independent predictive effect, ($\chi^2$(`r pgPredictN$Df[[2]]`) = `r round(pgPredictN$Chisq[[2]], 3)`, $p =$ `r max(.001, round(pgPredictN$"Pr(>Chisq)"[[2]], 3))`). This is likely because, as will be seen below, normality is jointly predicted by probability and morality, much of the variance in answer generation number is already accounted for by these ratings.

We sought to replicate the finding of [Bear and Knobe (2017)](https://doi.org/10.1016/j.cognition.2016.10.024) that normality judgments are predicted by judgments of morality and probability. First we constructed a linear mixed-effects model to predict normality ratings with probability and morality ratings as predictors and fixed effects for context and [not positive on how to refer to the structure of the model]. We further found that both ratings were independently predictive of normality. The model performed worse when probability was removed ($\chi^2$(`r pgNormP$Df[[2]]`) = `r round(pgNormP$Chisq[[2]], 3)`, $p <$ `r max(.001, round(pgNormP$"Pr(>Chisq)"[[2]], 3))`), and when morality was removed ($\chi^2$(`r pgNormM$Df[[2]]`) = `r round(pgNormM$Chisq[[2]], 3)`, $p <$ `r max(.001, round(pgNormM$"Pr(>Chisq)"[[2]], 3))`). Importantly, the model performed significantly worse when the interaction between morality and probability were removed ($\chi^2$(`r pgNormInt$Df[[2]]`) = `r round(pgNormInt$Chisq[[2]], 3)`, $p <$ `r max(.001, round(pgNormInt$"Pr(>Chisq)"[[2]], 3))`). 

```{r fig1, echo=F, warning=F, message=F, fig.width=6.5,fig.height=5.25}
## Figure 1
# Generation number vs value of option generated across three ratings

fig1 <- pgL %>% filter(question!="avg") %>%
  mutate(question = factor(question)) %>%
  mutate(answer=factor(answer)) %>%
  ggplot(aes(x=answer, y=response, fill=question)) +
  coord_cartesian(xlim = c(0.75, 5.25), ylim = c(0,100)) +
  geom_boxplot(position="dodge",alpha=.6,outlier.alpha = 0) +
  geom_point(aes(color=question),position=position_jitterdodge(jitter.width = .35), alpha = 0.1) +
  labs(x = "Generation number", y = "Value of option generated", fill="Rating:",color="Rating:") +
  geom_smooth(aes(y=response,x=as.numeric(answer),color=question),method="lm",position = position_dodge(width = .75)) +
  theme_bw() +
  theme(
    plot.background = element_blank()
    ,legend.position = "top"
    ,panel.grid.major = element_blank()
    ,panel.grid.minor = element_blank()
    ,axis.text.y=element_text(size=rel(1.4))
    ,axis.title=element_text(size=rel(1.5),vjust=.9)
    ,axis.text.x=element_text(size=rel(1.5))
    ,axis.ticks = element_blank()
    ,legend.text = element_text(size=rel(1.5))
    ,legend.title = element_text(size=rel(1.5))
  )

fig1 
# ggsave(fig1,file="../figs/fig1.png",width = 10,height = 7)

## Figure S1
figS1 <- perBest %>% 
  ggplot(aes(x = answer, y = perBest)) +
  labs(x = "Generation number", y = "Proportion of options generated that were best") +
  geom_point() +
  geom_line() +
  theme_bw() +
  theme(
    plot.background = element_blank()
    ,legend.position = "top"
    ,panel.grid.major = element_blank()
    ,panel.grid.minor = element_blank()
    ,axis.text.y=element_text(size=rel(1.4))
    ,axis.title=element_text(size=rel(1.5),vjust=.9)
    ,axis.text.x=element_text(size=rel(1.5))
    ,axis.ticks = element_blank()
    ,legend.text = element_text(size=rel(1.5))
    ,legend.title = element_text(size=rel(1.5))
  )

figS1 
# ggsave(figS1,file="../figs/figS1.png",width = 7,height = 7)

```

### Manually grouping participant responses

```{r tidycoded, warnings = F, echo=F,message=F}
## Read in codings of raters 1 and 2
# rater 1 codings
coding1 <- read.csv("../manualCoding/texts_coding_rater1.csv") %>% 
  rename(coding1 = group)
# rater 2 codings
coding2 <- read.csv("../manualCoding/texts_coding_rater2.csv") %>% 
  rename(coding2 = group)

coded <- full_join(coding1, coding2) 

## Caluclate inter-rater reliability
# Percent agreement:
perAgreementHum <- coded %>% 
  # filter(manual_group != 0) %>%
  summarise(agree = sum(coding1 == coding2)/n()) %>% pull()

# Cohen's Kappa
kappaHum <- kappa2(coded[, c("coding1", "coding2")])

## Read in rater 3 codings
# rater 3 rated in cases of disagreement between the first two raters
coding3 <- read.csv("../manualCoding/texts_coding_rater3.csv") %>% 
  rename(codingFinal = coded.final)

# put final coding results (rater 3 in cases of disagreement, rater 1 otherwise) in one dataframe
coded <- coded %>% 
  full_join(coding3 %>% select(-coded1, -coded2)) %>% 
  mutate(group = if_else(is.na(codingFinal), coding1, codingFinal)) %>% 
  select(-(3:5))

# link coded, which contains the text and manual grouping of each response, with pgL, which contains the participant ratings of their responses
coded <- coded %>% separate(scenario_id_fr, into = c("context", "id", "answer"), sep = "_") %>% 
  mutate(across(1:3, as.numeric)) %>% 
  right_join(pgL %>% filter(question!="avg") %>% group_by(context, id, answer) %>% summarise(value = mean(response)))

# read in coding key, which links numbers of each ratings with their label
codingKey <- read.csv("../materials/textsCodingKey.csv") %>% 
  rename(group = X)

# coded includes participant answers + ratings as well as manual coded groups
coded <- codingKey %>%
  pivot_longer(-group, names_to = "context", values_to = "groupText") %>% 
  mutate(across(1:2, parse_number)) %>%
  unite(col = "context_group", context, group, sep = "_") %>% 
  filter(groupText != "") %>%
  right_join(coded %>% mutate(context_group = paste(context, group, sep = "_"))) 

# coded %>% write.csv('../manualCoding/pg_coded_final.csv')

```

Two raters manually coded each of the `r nrow(coding1)` participant responses into 13-18 distinct action categories for each context. The criterion for a grouping was that at least three participants generated an action within that category. Two raters independently grouped participant responses with an inter-rater agreement of `r round(perAgreementHum * 100, 1)`% and a Cohen's kappa of `r round(kappaHum$value, 3)` indicating strong inter-rater reliability. A third rater determined final results in cases of disagreements. Action categories for each context are presented in [Table 2](#table-2).

#### Results

```{r codingresults, warnings = F, echo=F,message=F}
# ranks groups for each context by number of answers 
bestGroups <- coded %>% group_by(context, group) %>% 
  summarise(n = n(), mean = mean(value)) %>%
  filter(group != 0) %>% 
  group_by(context) %>% 
  mutate(rank = rank(-n, ties.method = "f")) %>% 
  unite("context_group", context, group)

# adds columns with information about whether a given answer is in the top 1 or 3 most picked groups for that context
coded <- coded %>% unite("context_group", context, group, remove = F) %>%
  ungroup() %>% 
  mutate(best = context_group %in%
           (bestGroups %>% filter(rank == 1) %>% pull(context_group)), 
         top3 = context_group %in%
           (bestGroups %>% filter(rank <= 3) %>% pull(context_group))) %>% 
  separate(context_group, into = c("context", "group"), sep = "_")

# percentage of participants who didn't come up with any of the most picked answers for that context
perBestCoded <- coded %>% 
  group_by(context, id) %>% 
  summarise(noneBest = !any(best),
            noneTop3 = !any(top3)) %>%
  ungroup() %>% 
  summarise(noneBest = sum(noneBest)/n(),
            noneTop3 = sum(noneTop3)/n())


# for each context do an independent samples t test comparing values of answers in the 3 most commonly used action categories to all other answers
t_table <- data.frame(context = numeric(0), t = numeric(0), df = numeric(0), p = numeric(0), mean_top3 = numeric(0), sd_top3 = numeric(0), mean_other = numeric(0), sd_other =numeric(0))
for (c in 1:18) {
  test <- coded %>% 
    filter(context == c) %>%
    with(t.test(value ~ top3))
  
  t_table <- bind_rows(t_table, 
        data.frame(context = c, t = test$statistic, df = test$parameter,
           p = test$p.value,
           mean_top3 = coded %>% filter(context == c, top3) %>% summarise(mean(value)) %>% pull(),
           sd_top3 = coded %>% filter(context == c, top3) %>% summarise(sd(value)) %>% pull(),
           mean_other = coded %>% filter(context == c, !top3) %>% summarise(mean(value)) %>% pull(),
           sd_other = coded %>% filter(context == c, !top3) %>% summarise(sd(value)) %>% pull(), n_top3 = coded %>% filter(context == c, top3) %>% nrow(), n_other = coded %>% filter(context == c, !top3) %>% nrow()))
}
rm(test)
```

We found striking convergence across participant's answers within each context. Only `r round(summarise(coded, otherPer = sum(group==0)/n())[[1]] * 100)`% of answers were labeled as 'other.' Across contexts, only `r round(perBestCoded[[1,2]] * 100)`% of participants did not come up with any of the 3 most common answers in their set of 5 possible actions, and less than `r round(perBestCoded[[1,1]] * 100)`% of participants didn't put down the most popular answer for that context.\
Not only did participants converge on a relatively small set of action categories (13-18), but the most common answers also tended to be rated highly. Across contexts, the actions in the three most common categories were rated higher ($M =$ `r coded %>% filter(top3) %>% summarise(mean(value)) %>% pull() %>% round(1)`, $SD =$ `r coded %>% filter(top3) %>% summarise(sd(value)) %>% pull() %>% round(1)`) than other actions ($M =$ `r coded %>% filter(!top3) %>% summarise(mean(value)) %>% pull() %>% round(1)`, $SD =$ `r coded %>% filter(!top3) %>% summarise(sd(value)) %>% pull() %>% round(1)`).\
This pattern held true within each context. For each context, we performed an independent samples t-test comparing average ratings for answers in the three most popular action categories to answers outside of these categories. For all but `r t_table %>% summarise(sum(p > .001)) %>% pull()` of the 18 contexts t-values were in the expected direction, with $p < .001$.


```{r codingHistograms, results = "asis", warning=F,message=F}
## Figure 2

# histogram showing the different action categories for a specific context , including information with number and average rating of actions in each category.
for (c in c(13)) { ## You can add contexts here if you want to see the corresponding graphs
  fig2 <- coded %>% 
  group_by(context, group, groupText) %>%
  summarise(count = n(), pgAvg = mean(value)) %>%
  filter(context == c) %>%
  group_by(context) %>% 
  mutate(rank = rank(-count, ties.method = "f"), groupText = as.factor(groupText)) %>% 
  ggplot(aes(x = reorder(groupText, rank), y = count, label = groupText,
             fill = pgAvg)) +
  labs(x = "Action category", y = "Number of participant responses", fill = "Average rating", title = paste("Context", c)) +
  geom_col() +
  theme_bw() +
  theme(
    plot.background = element_blank()
    #,legend.position = "none"
    ,panel.grid.major = element_blank()
    ,panel.grid.minor = element_blank()
    ,axis.text.y=element_text(size=rel(1.4))
    #,axis.title=element_blank()
    #,axis.title.y=element_blank()
    ,axis.text.x=element_text(size=rel(1.5), angle = 60, hjust = 1)
    ,axis.title.x = element_text(size=rel(2))
    ,axis.ticks = element_blank()
  )
  assign(paste0("codedHist", c), fig2)
}

# ggsave(fig2,file="../figs/fig2.png",width = 12,height = 10)

```

## Study 2: First-person decision making
### Participants
```{r participantsdecision, echo=F,warning=F,message=F}
# Read decision study data
decisionW <- read.csv("../data/decision.csv") %>% 
  rownames_to_column("id")

# List of ids of participants who gave nonsensical answers
exclude_ids <- c(21, 64, 72, 74, 84, 86, 89)
```

Critically, we are not only interested in finding a way of describing the space
of contextually relevant possibilities, but in demonstrating that these representations play a domain-general role in high-level cognition. We first tested
whether we could use this model of contextual possibility space to predict
responses in a first-person decision-making task. We modified our original 18
scenarios, replacing all names and pronouns referring to the agent with second-
person pronouns (see [Table 1](#table-1) for the modified scenarios). 
We collected a sample of `r length(unique(decisionW$id))` participants ($M_{age}$ = `r round(mean(decisionW$age,na.rm=T),2)`; $SD_{age}$ = `r round(sd(decisionW$age,na.rm=T),2)`; `r table(decisionW$gender)[[2]]` females) from Amazon Mechanical Turk ([www.mturk.com](www.mturk.com)). `r table(decisionW$finished)[[1]]` participants did not finish the study and we're excluded from further analysis. `r length(exclude_ids)` participants gave nonsensical/unclear responses, so their data was also excluded from analysis.

### Procedure
Each participant viewed all 18 contexts with modified pronouns in randomized order. After reading each context, participants were asked "In this situation, what would you decide to do?" and were given a free-response text box to write their answer within.

### Algorithmic grouping of participant responses
```{r tidydecision, echo=F,warning=F,message=F}

decisionL <- decisionW %>% 
  # remove participants who didn't finish the study
  filter(finished) %>% 
  select(id, S1_1:S18_1) %>% 
  # transform dataframe to long format
  pivot_longer(-id, names_to = "context", values_to = "decision",
               values_drop_na = T) %>%
  # select number for context names ("S18_1" -> 18)
  mutate(context = parse_number(context))

# remove participants who gave meaningless responses
# (Note: remove the negation to view their responses)
decisionL <- decisionL %>% filter(!(id %in% exclude_ids)) 
```

Instead of manually coding the first-person decision responses, we used natural language processing to group similar responses. First, we aggregated the responses from the original possibility generation study and the decision study, stripping all agent names and pronouns from the responses. We then computed sentence embeddings for all responses using [Sentence-BERT](www.sbert.net), reduced the dimensionality of the embedding space using [UMAP](umap-learn.readthedocs.io/en/), and performed k-means clustering on the resulting embeddings, with the number of clusters selected using the elbow method. The analysis used is provided in [nlpCoding/decision_NLP.ipynb](../nlpCoding/decision_NLP.ipynb). 

```{r clusterRead, echo=F,warning=F,message=F}
# Read in clustering results using two methods from decision_NLP.ipynb
cluster_tables_numManual <- list.files('../nlpCoding/numManual/cluster_tables', full.names = T)
cluster_tables_numElbow <- list.files('../nlpCoding/numElbow/cluster_tables', full.names = T)


# create a dataframe to aggregating all NLP clustering outputs 
clustering_outputs <- data.frame()
for (file in cluster_tables_numManual) {
  output <- read.csv(file, row.names="X")
  output <- output %>% mutate(clustering_method = "numManual")
  clustering_outputs <- bind_rows(clustering_outputs, output)
}
for (file in cluster_tables_numElbow) {
  output <- read.csv(file, row.names="X")
  output <- output %>% mutate(clustering_method = "numElbow")
  clustering_outputs <- bind_rows(clustering_outputs, output)
}


cluster_df <- clustering_outputs %>% 
  select(id, context, answer, response_original, source, value, clustering_method, cluster, cluster_name, 
         )

# filter to just responses from the possibility generation study, with clusters determined using the same number as the manual groupings
cluster_df_numManual <- cluster_df %>% 
              filter(source == 'pg' & clustering_method == 'numManual') %>%
              select(-c(source, value, response_original)) %>% 
              unite(key, id, context, answer)

# join the above dataframe with the manual groupings in order to compare the human and NLP clustering results
cluster_compare <- coded %>% 
  unite(key, id, context, answer, sep = "_") %>%
  rename(response = text, manual_group = group) %>% 
  left_join(y = cluster_df_numManual,
            by = join_by(key)) %>% 
  separate(key, into = c("id", "context", "answer")) %>% 
  select(context, id, answer, response, 
         manual_group, cluster, groupText, cluster_name, value) %>% 
  mutate(context= as.integer(context))

# For each cluster, finds manual group # with the highest shared number of responses
biggest_manual_group_by_cluster <- cluster_compare %>% 
  group_by(context, cluster, 
           cluster_name,
           manual_group, 
           groupText
           ) %>% 
  summarise(n = n(), .groups = "keep") %>% # calculates number of each manual group to NLP context pair
  group_by(context, cluster) %>% 
  arrange(desc(n)) %>% # within each cluster, sorts by number
  slice(1) %>% # select manual group with highest number of responses in each cluster
  select(-n) %>% 
  rename(biggest_manual_group_by_cluster = manual_group) 

# Viewing this dataframe suggests allignment in the manual and NLP groupings
cluster_compare <- cluster_compare %>% 
  # left_join(manual_group_of_centroid %>% select(-cluster_name), by = join_by(context, cluster)) %>% 
  left_join(biggest_manual_group_by_cluster %>% select(-groupText), by = join_by(context, cluster, cluster_name)) 




## Inter-rater reliability of manual grouping and clustering algorithm 

# Percent agreement:
perAgreementNLP <- cluster_compare %>% 
  # filter(manual_group != 0) %>%
  summarise(
    # group_same_as_cluster_centroid = sum(manual_group == manual_group_of_centroid)/n(),
            group_same_as_majority_of_cluster = sum(manual_group == biggest_manual_group_by_cluster)/n()) %>% pull()

cluster_compare <- cluster_compare %>% 
  mutate(biggest_manual_group_by_cluster = as.factor(biggest_manual_group_by_cluster),
         manual_group = as.factor(manual_group)) 

kappaNLP <- kappa2(cluster_compare[, c("biggest_manual_group_by_cluster", "manual_group")])


```

Because we used K-means clustering, we had to specify the number of clusters to be used for each scenario. First we set the number of clusters for each context to be identical to the number of manual groups used for that context, in order to directly compare the performance of our manual grouping and the algorithmic clustering.

Qualitatively, the clusters generated were semantically meaningful, with similar types of answers being grouped together, even when the exact words differed (for example, see [cluster plot for context 13 ](../nlpCoding/numSameAsManual/plots/S13.html)). We further calculated the agreement between the clustering method and our manual groupings. Each cluster was assigned to a manual grouping by determining which manual grouping accounted the largest amount of responses within the cluster. Note that because the manual grouping method and the NLP clustering segmented semantic space differently, two different clusters could be linked to the same manual grouping if a plurality of responses in both clusters were grouped together. A given response was manually coded into the same manual group as the plurality of its cluster `r round(perAgreementNLP * 100,1)`% of the time. Treating the manual grouping method and the clustering method as two different raters we found a Cohen's kappa coefficient of `r round(kappaNLP$value,3)` suggesting strong agreement between the clustering and the manual grouping methods.

Having verified that the clustering method generated similar results as human raters when using the same number of clusters, we concluded that the clustering method was effective at identifying semantically meaningful action categories. However, for further analysis, we sought to use a more data-driven approach to determining the appropriate number of clusters. For each context, we generated an 'elbow plot' to determine the appropriate number of clusters. For instance the [elbow plot for context 13 ](../nlpCoding/numElbow/elbow_plots/S13.png) was assessed as suggesting an optimal number of clusters at k = 7. We then re-ran the clustering analysis used above with the k-values obtained from these elbow plots.

### Results
```{r decisionpredict, echo=F,warning=F,message=F}
## Here we model participant behavior in the first-person decision study

# filter to clustering results obtained using k as determined by elbow method for k-means
cluster_df_elb <- cluster_df %>% filter(clustering_method == 'numElbow')

# calculate average value of possibility generation responses for each cluster
pg_clusters <- cluster_df_elb %>%
  filter(source == 'pg') %>% 
  group_by(context, cluster, cluster_name) %>% 
  summarise(avg_value_pg = mean(value), n_pg = n(), .groups = "drop") %>% 
  group_by(context) %>% 
  mutate(proportion_pg = n_pg/sum(n_pg)) 

# calculate proportion of decision responses for each context being grouped into each cluster
decision_clusters <- cluster_df_elb %>% 
  filter(source == 'decision') %>% 
  group_by(context, cluster, cluster_name) %>% 
  summarise(n_decision = n(), .groups = "drop") %>% 
  group_by(context) %>% 
  mutate(proportion_decision = n_decision/sum(n_decision)) 


## Model:
### Step 1: Sample k samples from pg for a given context.
### Step 2: select response  with highest cluster value.
### Repeat 1000 times, summarise to get proportion of times each cluster is selected for a given scenario

# Function to perform the decision model for a given k
run_decision_model <- function(k, df, pg_clusters, sample_sets) {
  print(paste("k =", k))
  
  decision_model <- df %>% 
    filter(source == 'pg' & clustering_method == 'numElbow') %>%
    left_join(pg_clusters, by = join_by(context, cluster, cluster_name)) %>% 
    group_by(context) %>%
    # sample k * sample_sets responses with replacement from each context
    sample_n(k * sample_sets, replace = TRUE) %>%
    # split resulting responses into sample sets of k size
    mutate(sample_set = (row_number() - 1) %/% k) %>%
    group_by(context, sample_set) %>%
    # take the response with the highest avg value of cluster
    arrange(desc(avg_value_pg)) %>%
    slice(1) %>%
    group_by(context, cluster) %>%
    # summarise likelihood of being selected for each cluster within each context
    summarise(model_proportion = n() / sample_sets, .groups = "drop")
  
  # join decision model with pg_clusters and decision_clusters add contexts that were never sampled by the model
  clusters_summarized <- left_join(pg_clusters, decision_model,
                                   by = join_by(context, cluster)) %>%
    left_join(decision_clusters, by = join_by(context, cluster, cluster_name)) %>% 
    # replace NA values (clusters that were never sampled) with 0
    mutate(model_proportion = if_else(is.na(model_proportion), 0, model_proportion),
           proportion_decision = if_else(is.na(proportion_decision), 0, proportion_decision)) %>% 
    mutate(k = k)
  
  return(clusters_summarized)
}

# Specify the range of k values
k_values <- 1:10
# Specify how many consideration sets are sampled
sample_sets = 10000


# Check if decision results has already been computed, if so uses the dataframe stored
if(file.exists("../computationOutputs/decisionResults.rda")){
  decision_results <- readRDS(file = "../computationOutputs/decisionResults.rda")
} else {
  # Otherwise run the decision model for each k value
  decision_results <- map_df(k_values,
                             ~ run_decision_model(.x, cluster_df, 
                                                  pg_clusters, sample_sets))
  
  # Store results in computationOutputs
  decision_results %>% saveRDS(file = "../computationOutputs/decisionResults.rda")
}


# table with model correlations and p values for 1 <= k <= 10
k_correlations <- decision_results %>% group_by(k) %>%
  summarise(correlation = cor.test(model_proportion, proportion_decision)$estimate,
            p_value = cor.test(model_proportion, proportion_decision)$p.value)

# the k-value from the above correlations table with the highest correlation is assessed as the optimal consideration-set size
optimal_k <- k_correlations %>% arrange(p_value) %>% slice(1) %>% pull(k)

# selects results from decision model with k=2 as determined by above analysis
cor_result <- with(decision_results %>% filter(k==optimal_k), cor.test(model_proportion, proportion_decision))
```
We constructed a model to predict participants' decisions in this task, seeking to predict the likelihood of responses within each cluster being selected as participant's decisions. In-keeping with pre-existing two-step models of decision-making ([Morris et al.](https://doi.org/10.1177/09567976211005702)), the model first constructed a consideration set by sampling actions from participants' responses to [Study 1](#study-1). It then ranked actions within each consideration set using average participant ratings for the cluster each action fell into, with the highest ranked action within each set being selected as the final decision. This process was repeated 10,000 times for each context, allowing us to calculate the likelihood of an action from each cluster being chosen as the model's decision.

We ran this analysis with consideration set size varying from 1-10. At consideration size $k = 1$, the model proportion for a cluster is equal to the likelihood of initially sampling that cluster. At all other consideration set sizes, the average value of the cluster also plays a role. As consideration size increases, initial sampling likelihood plays a less significant role and average value is more significant, with low-probability but high-value actions more likely to be selected by the model.

The model significantly predicted participant responses at all considerations set sizes tested. The model performed best at $k =$ `r optimal_k`, with a correlation of $r =$ `r k_correlations %>% summarise(max(correlation)) %>% pull() %>% round(3)`, $p <$ `r max(.001, k_correlations %>% summarise(min(p_value)) %>% pull())`, and worst at $k =$ `r k_correlations %>% filter(correlation == min(correlation)) %>% pull(k)` with a correlation of $r =$ `r k_correlations %>% summarise(min(correlation)) %>% pull() %>% round(3)`, $p <$ `r max(.001, k_correlations %>% summarise(max(p_value)) %>% pull())`. This aligns with existing research suggesting that people tend to only consider a few options when making decisions. 

```{r fig3, warning=F, message=F, fig.width=6.5,fig.height=5.25}
# Figure 3
# Decision model results

fig3 <- decision_results %>% 
  # filter to optimal cluster size (k=2)
  filter(k == optimal_k) %>% 
  mutate(context = as.factor(context)) %>% 
  ggplot(aes(x = model_proportion, y = proportion_decision)) +
  geom_point(aes(color = context, group = cluster_name)) +
  geom_smooth(method = "lm") +
  labs(x = "Modeled Decision Likelihood", y = "Actual Decision Likelihood",) +
  theme_bw() +
    theme(
      plot.background = element_blank()
      ,panel.grid.major = element_blank()
      ,panel.grid.minor = element_blank()
      # ,legend.title=element_blank()
      ,legend.text=element_text(size=rel(1.4))
      # ,axis.text=element_text(size=rel(2))
      # ,axis.title.y=element_text(vjust=.9)
      ,axis.ticks = element_blank()
      ,strip.text=element_text(size=rel(1))
      ,axis.title=element_text(size=rel(1.5))
     # ,legend.position = "none"
    )

fig3

# ggsave(fig3,filename = "../figs/fig3.png", width = 10, height = 8,units = "in")

```



## Study 3: Action Ratings
For each context, we created a list of six actions the agent could possibly do. These actions were selected to vary widely along the space of options that come to mind. The six actions for each context are presented in [Table 1](#table-1).

### Participants

```{r participantsev, echo=F,warning=F,message=F}
evW <- read.csv("../data/ev.csv") %>% 
  rownames_to_column("id")
```

To verify that the actions we came up with did in fact vary as intended, we collected a sample of `r length(unique(evW$id))` participants ($M_{age}$ = `r round(mean(evW$age,na.rm=T),2)`; $SD_{age}$ = `r round(sd(evW$age,na.rm=T),2)`; `r table(evW$gender)[[2]]` females) from Amazon Mechanical Turk ([www.mturk.com](www.mturk.com)) to rate the actions.

### Procedure

Each participant viewed a random subset of six of the eighteen contexts. For each context, in addition to reading the story, they were randomly assigned one of the six actions and told that the agent was considering that action. They were then asked to rate that action on the same three 100-point scales used earlier, measuring the probability, morality and normality of the action.

### Results
```{r tidyev, echo=F,warning=F,message=F}
# creates long dataframe
evL <- evW %>% filter(Progress==100) %>% 
  select(id, S1_4:S18_6) %>% 
  pivot_longer(-id, names_to = "context_question", values_to = "response",
               values_drop_na = T) %>% 
  separate(context_question, into = c("context", "question"), sep = "_") %>% 
  mutate(across(1:3, parse_number)) %>% 
  mutate(question = if_else(question == 4, "Probability", 
                            if_else(question == 5, "Morality", "Normality")))

evL <- evW %>% select(id, S1A:S18A) %>% 
  pivot_longer(-id, names_to = "context", values_to = "action") %>% 
  mutate(context = parse_number(context),
         id = as.numeric(id)) %>% 
  left_join(evL, .) %>%
  relocate(action, .after = "context") %>%
  pivot_wider(names_from = question, values_from = response ) %>%
                mutate(avg=(Normality+Probability+Morality)/3) %>% # this creates the average for all three judgments
                pivot_longer(cols=c(Normality,Morality,Probability,avg),
                             names_to = "question",values_to ="response")

# creates a summary table with average ratings for each action
evSum <- evL %>% 
  group_by(context, action, question) %>%
    summarise(mean = mean(response), .groups = "drop") %>%
    as.data.frame() %>% 
    group_by(context) %>% 
    mutate(actionNumb = paste(context, as.integer(as.factor(action)), sep = "_") ) %>% 
  relocate(actionNumb, .after = "context") 

```
The 'actual actions' we generated varied widely across all contexts. We averaged participant responses for each action as our measure of action value for each dimension. The average probability ratings of the actions ranged from `r evSum %>% filter(question == 'Probability') %>% pull(mean) %>% min() %>% round(2)` to `r evSum %>% filter(question == 'Probability') %>% pull(mean) %>% max() %>% round(2)`, $M =$ `r evSum %>% filter(question == 'Probability') %>% pull(mean) %>% mean() %>% round(2)`, $SD =$ `r evSum %>% filter(question == 'Probability') %>% pull(mean) %>% sd() %>% round(2)`, the average morality ratings ranged from `r evSum %>% filter(question == 'Morality') %>% pull(mean) %>% min() %>% round(2)` to `r evSum %>% filter(question == 'Morality') %>% pull(mean) %>% max() %>% round(2)`, $M =$ `r evSum %>% filter(question == 'Morality') %>% pull(mean) %>% mean() %>% round(2)`, $SD =$ `r evSum %>% filter(question == 'Morality') %>% pull(mean) %>% sd() %>% round(2)`, and the average normality ratings ranged from `r evSum %>% filter(question == 'Normality') %>% pull(mean) %>% min() %>% round(2)` to `r evSum %>% filter(question == 'Normality') %>% pull(mean) %>% max() %>% round(2)`, $M =$ `r evSum %>% filter(question == 'Normality') %>% pull(mean) %>% mean() %>% round(2)`, $SD =$ `r evSum %>% filter(question == 'Normality') %>% pull(mean) %>% sd() %>% round(2)`.

We averaged all three ratings for each action and took this average as a measure of an action's 'value.'

```{r histograms, echo = F}
## Figure S2
# histogram showing the distribution of participant response ratings along with the average ratings for the events we gave to participants
figS2 <- pgL %>% #filter(question!="avg") %>%
  mutate(question = factor(question),
         question = factor(c("Value","Morality","Normality","Probability")[question])) %>%
    ggplot(aes(x = response, fill = question, color = question)) +
      geom_histogram(position = "identity",
                     alpha = 0.33,
                     bins = 15) +
      geom_jitter(data = evSum %>% #filter(question!="avg") %>%
                          mutate(question = factor(question),
                          question = factor(c("Value","Morality","Normality","Probability")[question]))
                  ,size = 1.5
                  ,aes(x = mean, y = -250)
                  ,width = 0, height = 100
                  ,alpha = .33) +
      facet_grid(~ question) +
    theme_bw() +
    theme(
      plot.background = element_blank()
      ,panel.grid.major = element_blank()
      ,panel.grid.minor = element_blank()
      ,legend.title=element_blank()
      ,legend.text=element_text(size=rel(1.4))
      ,axis.text.y=element_text(size=rel(1.5))
      ,axis.title.y=element_text(vjust=.9)
      ,axis.ticks = element_blank()
      ,strip.text=element_text(size=rel(1))
      ,axis.title=element_text(size=rel(1))
     ,legend.position = "none"
    ) +
    xlab("Value") +
    ylab("Count")

figS2
# ggsave(figS2,filename = "../figs/figS2.png", width = 7, height = 5,units = "in")

```


### Calculating the modal distance
In order to predict participants' higher level judgments for these generated actions, we developed a novel measure that compared ratings for each action against the set of all participant generated responses for the context. This value, what we're referring to as the *modal distance*, is the proportion of participant generated responses for the context (from [Study 1](#study-1)) that had a higher average rating than the average rating for a given action (from [Study 2](#study-2)). The less normal an answer is, the higher its modal distance value will be (i.e. the more distant it is from the center of the contextual modal space). Importantly, the value is context dependent: an action with an average normality rating of 50 will have a lower modal distance in a context with skewed positive responses than in a context with a more uniform distribution. This value will be used to predict responses in all subsequent studies. \ 
We also sought to see whether each individual judgment (probability, morality and normality) was separately predictive of high-level judgments. We recalculated the modal distance for each action, except instead of using the averages of probability, morality and normality as the value score for each participant-generated and actual action, we calculated three different scores, one for each type of judgment. This gave use a probability distance, a morality distance and normality distance for each action.

```{r modalDistancePlot, message=F,warning=F,echo=F}
## Figure 4
# Figure showing how modal distance is calculated
density13 <-  pgL %>% 
  filter(context == 13) %>% ## just using our example context
  filter(question == "avg") %>% ## using the average measure
  mutate(disvalue=100-response) %>%
    ggplot(aes(x = disvalue),color="grey") +
      geom_histogram(aes(y = ..density..),
                     position = "identity",
                     alpha = 0.33,
                     bins = 20) +
      geom_density(lwd = 1, alpha = 0.25, fill="grey") +
      geom_label_repel(data = evSum %>% filter(context == 13) %>% ## again only context 13 for the actual actions
                                     filter(question=="avg") %>% ## again using the average of the judgments
                                     mutate(disvalue=100-mean,
                                            event=factor(action),
                                            shortevent = factor(c("blame maid of honor",
                                                                  "borrow ring",
                                                                  "call taxi",
                                                                  "run away",
                                                                  "steal ring",
                                                                  "tell sister")[event])
                                            ), 
                 aes(x = disvalue, y = -0.006, 
                     label = shortevent, color=c("black","black","black","red","black","black")),
                 check_overlap=TRUE,
                 position=position_jitter(width = 0, height = .001,seed=4),
                 segment.colour = NA
                ) +
    scale_color_manual(values=c("black","red")) +
    xlab("Disvalue Score") +
    ylab("Density") +
    #ggtitle("Context 13 (Daniel the Ring Bearer)") +
    theme_bw() +
    theme(
      plot.background = element_blank()
      ,panel.grid.major = element_blank()
      ,panel.grid.minor = element_blank()
      ,legend.title=element_blank()
      ,legend.text=element_text(size=rel(1.25))
      ,axis.text.y=element_text(size=rel(1))
      ,axis.title.x=element_text(size=rel(1.25))
      ,axis.title.y=element_text(vjust=.9,size=rel(1.25))
      ,axis.ticks = element_blank()
      ,strip.text=element_text(size=rel(1))
      ,axis.title=element_text(size=rel(1))
     ,legend.position = "none"
     ,plot.title = element_text(hjust = 0.5)
    ) 

# Actual action for context 13 to select (get x value)
infoA <- ggplot_build(density13)$data[[3]] ## these are the actual action points
infoC <- ggplot_build(density13)$data[[2]] ## this is the density curve

c13h.xmax <- infoA$x[4] ## chose a high abnormality actual action
c13h.info <- infoC %>% filter(x < c13h.xmax)

c13.h <- density13 + 
         geom_segment(data=data.frame(), ##otherwise you draw this segment for every row of data...
                      aes(x = 30, y = .021, xend = 45, yend = .0025),
                  arrow = arrow(length = unit(0.45, "cm")),
                  color="red", alpha=.35, linetype=1) +
          geom_vline(xintercept=c13h.xmax,alpha=.5,linetype=3,color="red") +
          geom_area(data = c13h.info, aes(x=x, y=y), fill="red",alpha=.25) +
          annotate(geom="text", x=30, y=.022, label="Modal Distance(running away)",
            color="red",alpha=.65) 

# ggsave(c13.h,file="../figs/fig4.png", width=8, height=6, units="in")

```

```{r modalDistance, message=F,warning=F,echo=F}
### Here where we calculate contextual modal distance for every action
modalDistTable <- data.frame()

for (c in unique(pgL$context)){
  
  numbSamples <- 10000
  
  pgSamp <- data.frame(sampId = seq(1,numbSamples))
  
  # samples 10,000 values from averages of participant responses for that context
  pgSamp$avg <- pgL %>%
    filter(context == c, question == 'avg') %>% 
    pull(response) %>% 
    sample(numbSamples, replace = T)  
  
  # samples 10,000 morality ratings from participant responses
  pgSamp$moral <- pgL %>%
    filter(context == c, question == 'Morality') %>% 
    pull(response) %>% 
    sample(numbSamples, replace = T)
  
  # samples 10,000 probability ratings from participant responses
  pgSamp$prob <- pgL %>%
    filter(context == c, question == 'Probability') %>% 
    pull(response) %>% 
    sample(numbSamples, replace = T)
  
  # samples 10,000 normality ratings from participant responses
  pgSamp$norm <- pgL %>%
    filter(context == c, question == 'Normality') %>% 
    pull(response) %>% 
    sample(numbSamples, replace = T) 
  
  
  # for each action, calculates the proportion of samples that are rated higher on average, higher by morality, higher by probability, and higher by normality than that action
  for (a in 1:6){
    modalDistTable <- evSum %>%
      filter(context == c) %>% 
      pivot_wider(names_from = question, values_from = mean) %>% 
      rename(mean = avg, moralityMean = Morality, probabilityMean = Probability, normalityMean = Normality) %>% 
      slice(a:a) %>%
      mutate(
        # proportion of samples that are rated higher on average than given action
        modalDistance = length(pgSamp$avg[mean<pgSamp$avg])/length(pgSamp$avg), 
          # proportion of samples that are rated higher by morality
              moralDistance = length(pgSamp$moral[moralityMean<pgSamp$moral])/length(pgSamp$moral), 
        # proportion of samples that are rated higher by morality
            probDistance = length(pgSamp$prob[probabilityMean<pgSamp$prob])/length(pgSamp$prob),
        # proportion of samples that are rated higher by normality
            normDistance = length(pgSamp$norm[normalityMean<pgSamp$norm])/length(pgSamp$norm)) %>%
      bind_rows(modalDistTable)
  }
  
}
```

## Study 4: Force judgments

### Participants
```{r participantsht, echo=F,warning=F,message=F}
# read in participants' 'had to' judgments for each action
htW <- read.csv("../data/hadTo.csv")%>% 
  rownames_to_column("id")
```

We collected a sample of `r length(unique(evW$id))` participants ($M_{age}$ = `r round(mean(htW$age,na.rm=T),2)`; $SD_{age}$ = `r round(sd(htW$age,na.rm=T),2)`; `r table(htW$gender)[[2]]` females) from Amazon Mechanical Turk ([www.mturk.com](www.mturk.com)).

### Procedure

Each participant was randomly presented with 12 of the 18 contexts. For each context, participants were randomly assigned one of the six actions and told that the agent decided to do that action. They then rated their agreement with a statement that the agent was forced to complete the action on a 100 point scale.

|     *Force statement:* [Agent] had to do [Action].

### Results

```{r tidyht, echo=F,warning=F,message=F}
# creates long table
htL <- htW %>% select(id, S1_1:S18_1) %>% 
  pivot_longer(-id, names_to = "context", values_to = "response", 
               values_drop_na = T) %>% 
  mutate(context = parse_number(context))

# adds information about which action each participant viewed for each context
htL <- htW %>% select(id, S1A:S18A) %>% 
  pivot_longer(-id, names_to = "context", values_to = "action",
               values_drop_na = T) %>% 
  mutate(context = parse_number(context)) %>% 
  filter(action != "") %>% 
  right_join(htL)  %>% 
  filter(context != 11) # removed context 11 because of a typo in the question

# creates summary table with average 'had to' rating for each action
htSum <- htL %>% group_by(context, action) %>% 
  summarise(hadTo = mean(response)) %>% 
  mutate(actionNumb = paste(context, as.integer(as.factor(action)), sep = "_")) %>% 
  ungroup()

# joins with modal distance table to correlate each action
htSum <- htSum %>% right_join(modalDistTable %>% select(-action)) %>% 
  relocate(hadTo, .after = modalDistance) %>% 
  filter(context != 11) 

# modal distance correlations with 'had to' judgments
htCorr <- htSum %>% with(list(cor.test(modalDistance, hadTo),
               cor.test(moralDistance, hadTo),
               cor.test(probDistance, hadTo),
               cor.test(normDistance, hadTo)))
```

Due to a typo in the question for context 11 (the question referred to a different agent than the main context text), results for this context were excluded from analysis.

Across all actions, participants reported a wide range of agreement with statements of force attribution ($M =$ `r htL %>% summarise(mean(response)) %>% pull() %>% round(1) %>% format(nsmall = 1)`, $SD =$ `r htL %>% summarise(sd(response)) %>% pull() %>% round(1) %>% format(nsmall = 1)`). As expected, using the modal distance value to predict average force judgments for each action, we found a correlation of $r =$ `r round(htCorr[[1]]$estimate[[1]], 3)`, $p <$ `r max(.001, round(htCorr[[1]]$p.value[[1]], 3))`.\ 
The moral distance had a correlation of  $r =$ `r round(htCorr[[2]]$estimate[[1]], 3)`, $p <$ `r max(.001, round(htCorr[[2]]$p.value[[1]], 3))` with force judgments; probability distance had a correlation of  $r =$ `r round(htCorr[[3]]$estimate[[1]], 3)`, $p <$ `r max(.001, round(htCorr[[3]]$p.value[[1]], 3))` and normality distance had a correlation of  $r =$ `r round(htCorr[[4]]$estimate[[1]], 3)`, $p <$ `r max(.001, round(htCorr[[4]]$p.value[[1]], 3))`.

```{r fig5, echo=F,warning=F,message=F}

fig5 <- htSum %>% mutate(context=factor(context)) %>%
                  ggplot(aes(y=hadTo,x=modalDistance)) +
                  geom_point(aes(color=context)) +
                  labs(y="Agreement agent was forced", 
                       x="Modal distance of actual action") +
                  geom_smooth(method=lm)+
                  theme_bw() +
                  theme(
                    plot.background = element_blank()
                    #,legend.position = "top"
                    ,panel.grid.major = element_blank()
                    ,panel.grid.minor = element_blank()
                    ,axis.text=element_text(size=rel(1.25))
                    #,axis.title=element_blank()
                    #,axis.title.y=element_blank()
                    #,axis.text.x=element_text(size=rel(1.5))
                    ,axis.title = element_text(size=rel(1.25))
                    ,axis.ticks = element_blank()
                  )

fig5

# ggsave(fig5,file="../figs/fig5.png",width = 10,height = 8)

```

## Study 5: Causal judgments
For each context, we came up with one downstream consequence that could potentially occur regardless of which of the six actions the agent decided to. Downstream consequences can be found in [Table 1](#table-1). We sought to predict participants' ratings of whether the action chosen by the agent caused these downstream consequences using our existing modal distance value. These judgments don't just reflect participant's judgments of the actual action but also the relationship between the action and downstream consequence. We also sought to compare our model to an existing model of causal judgments that involves judgments of whether a potential cause was sufficient for the outcome. We collected data in three studies, the first on causal judgments, the second on counterfactual relevance and necessity ratings of the action and the third on the sufficiency ratings.	

### Study 5a: Causal judgments

#### Participants

```{r participantscausal, echo=F,warning=F,message=F}
# reads in data for participants' ratings of whether the agent caused the downstream consequence
causalW <- read.csv("../data/causal.csv") %>% 
  rownames_to_column("id")

# reads in data for participants' ratings of whether the action was necessary for causing the outcome and for the relevancy of alternative events
counterW <- read.csv("../data/causal+counterfactual.csv") %>% 
  rownames_to_column("id") %>% 
  rename_with(~gsub("\\.", "_", .x), .cols = S1_1_1:S18.2_1)

# reads in data for participant ratings of whether the action was sufficient for the downstream consequence
suffW <- read.csv("../data/sufficiencyCounterfactual.csv") %>% 
  rownames_to_column("id") %>% 
  rename_with(~gsub("\\.", "_", .x), .cols = S1_1_1:S18.1_1)

```

For the first study, we collected a sample of `r length(unique(causalW$id))` participants ($M_{age}$ = `r round(mean(causalW$age,na.rm=T),2)`; $SD_{age}$ = `r round(sd(causalW$age,na.rm=T),2)`; `r table(causalW$gender)[[2]]` females) from Amazon Mechanical Turk ([www.mturk.com](www.mturk.com)).

#### Procedure

Each participant viewed a randomized subset of 12 of the 18 contexts. For each context, participants read the story and were told that the agent decided to do one of the six actions. They then read that afterwards, another event happened. Participants were then asked to rate their agreement with the following statement on a 100 point scale:

|     *Causal statement:* [Agent] caused [Downstream consequence].

### Study 5b: Counterfactual judgments

#### Participants

For the second study, we collected a sample of `r length(unique(counterW$id))` participants ($M_{age}$ = `r round(mean(counterW$age,na.rm=T),2)`; $SD_{age}$ = `r round(sd(counterW$age,na.rm=T),2)`; `r table(counterW$gender)[[2]]` females) from Amazon Mechanical Turk ([www.mturk.com](www.mturk.com)).

#### Procedure

Participants viewed a randomized subset of 12 of the 18 contexts. For each context, participants read the story and were told that the agent decided to do one of the six actions. They then read that afterwards, another event happened. Participants were then asked to rate their agreement with the following statements on 100 point scales:

|     *Counterfactual relevance statement:* Given the situation they were in, how relevant is it to consider the possibility that [Agent] could have done
|     something other than deciding to [Chosen action].

|     *Necessity statement:* If [Agent] hadn't decided to [Chosen action], [Downstream consequence] wouldn't have occurred.
| \ 
These statements were modified as necessary to make the sentence flow naturally (e.g. "If Heinz hadn't decided to [Chosen action], his wife wouldn't have gotten more ill.")

### Study 5c: Sufficiency judgments	

#### Participants	
For a third study, we collected a sample of `r length(unique(suffW$id))` participants ($M_{age}$ = `r round(mean(suffW$age,na.rm=T),2)`; $SD_{age}$ = `r round(sd(suffW$age,na.rm=T),2)`; `r table(suffW$gender)[[2]]` females) from Amazon Mechanical Turk ([www.mturk.com](www.mturk.com)).	

#### Procedure	
Participants viewed a randomized subset of 12 of the 18 contexts. For each context, participants, read the story and were told that the agent decided to do one of the six actions. They then read that afterwards, another event happened. Participants were asked to rate their agreement with the following statement on a 100-point scale:

|     *Sufficiency statement:* Given that [Agent] decided to [Chosen action], [Downstream consequence] was going to happen.
| \ 
These statements were modified as necessary to make the sentence flow naturally (e.g. "Given that Heinz decided to [Chosen action], his wife was going to get more ill.")


### Results

```{r tidycausal, warning=F,message=F,echo=F}
# turns table to long
causalL <- causalW %>% filter(Progress == 100) %>% 
  select(id,S1_1:S18_1) %>% 
  pivot_longer(-id, names_to = "context", values_to = "response", values_drop_na = T) %>% 
  mutate(across(1:2, parse_number)) 

# Join with info about which action was being judged in each context
causalL <- causalW %>% filter(Progress == 100) %>% 
  select(id,S1A:S18A) %>% 
  pivot_longer(-id, names_to = "context", values_to = "action") %>% 
  filter(action != "") %>% 
  mutate(across(1:2, parse_number)) %>% 
  right_join(causalL)

# Read in the necessity judgments for each action/outcome pair
counterL <- counterW %>% filter(Progress == 100) %>% 
  select(id,S1_1_1:S18_2_1) %>% 
  pivot_longer(-id, names_to = "context_question", values_to = "response", values_drop_na = T) %>% 
  separate(context_question, into = c("context", "question", "null")) %>% 
  select(-null) %>% 
  mutate(across(1:2, parse_number), question = as.factor(question)) %>% 
  filter(id > 30) %>% # removes participants in pilot
  mutate(question = recode_factor(question, "1" = "counterfactual_relevance", "2" = "counterfactual_necessity"))

# Join with info about which action was being judged in each context
counterL <- counterW %>% filter(Progress == 100) %>% 
  select(id,S1A:S18A) %>% 
  pivot_longer(-id, names_to = "context", values_to = "action") %>% 
  filter(action != "") %>% 
  mutate(across(1:2, parse_number)) %>% 
  filter(id > 30) %>% # removes participants in pilot
  right_join(counterL)

# Read in the sufficiency judgments for each action/outcome pair
suffL <- suffW %>% filter(Progress == 100) %>% 
  select(id,S1_1_1:S18_1_1) %>% 
  pivot_longer(-id, names_to = "context", values_to = "response", values_drop_na = T) %>%
  filter(response != "") %>%
  mutate(across(1:2, parse_number))


# Join with info about which action was being judged in each context
suffL <- suffW %>% filter(Progress == 100) %>% 
  select(id,S1A:S18A) %>% 
  pivot_longer(-id, names_to = "context", values_to = "action") %>% 
  filter(action != "") %>% 
  mutate(across(1:2, parse_number)) %>% 
  right_join(suffL)



# bringing these all together at the level of each 6 action-outcome pairs for all 18 contexts
causalSum <- counterL %>% 
   group_by(context, action, question) %>% 
   summarise(mean = mean(response)) %>% 
   pivot_wider( names_from = question, values_from = mean) %>% 
   group_by(context) %>% 
  mutate(actionNumb = paste(context, row_number(), sep = "_"), .after = context) %>% 
  right_join(suffL %>% group_by(context,action) %>%
               summarise(suffCF = mean(response))
  ) %>%
  right_join(causalL %>% group_by(context, action) %>% 
                         summarise(causal = mean(response))
  ) %>% 
  right_join(modalDistTable %>% select(-c(action, mean))) %>% 
      # necessity strength = modalDistance (relevance of alternative events) * counterfactual_necessity (would the downstream consequence have occurred if the action hadn't happened?)
  mutate(necessity_strength = modalDistance * counterfactual_necessity)


##  Here, we estimate the probability of participants sampling an event in which E=a_a, or the probability of sampling the actual action done for each actual action.

# read in coding of each actual action into our manual coding groups
actualActions <- read.csv("../materials/actualActions.csv", row.names = "X") 

causalSum  <- causalSum %>% arrange(context, actionNumb) %>%
  bind_cols(actualActions %>% arrange(context, actionNumb) %>% select(group))

causalSum <- causalSum %>% group_by(context, group)%>% 
  mutate(probSamp = 
                      if_else(group == 0, 0, # if an actual action was not in coded as part of an action group, it has a probability of sampling of 0
                              nrow(coded[coded$context == context & coded$group == group,])/nrow(coded[coded$context == context,])) # otherwise, actions are given a value equal to the proportion of responses for the context in the same action category
         ) %>% 
  ungroup() %>% 
  mutate(suff_strength = probSamp * suffCF, # multiply probability of sampling an action in the same category by participant ratings that the action was sufficient for the downstream consequence.
         icard_model = necessity_strength + suff_strength, # This is the Icard model full causal strength term
        icard_modified = necessity_strength + suffCF)

potencyCorr <- with(causalSum, cor.test(necessity_strength, causal))
causalCorr <- with(causalSum, list(cor.test(moralDistance, causal),
                     cor.test(probDistance, causal),
                     cor.test(normDistance, causal)))
modelCorr <- with(causalSum, cor.test(suff_strength,necessity_strength))
suffCorr <- with(causalSum, cor.test(suff_strength,causal))


# full model
lmer0 <- lmer(data = causalSum,
              causal ~ counterfactual_necessity * modalDistance + (1|context))
# model with interaction removed
lmer1 <- lmer(data = causalSum,
     causal ~ counterfactual_necessity + modalDistance + (1|context))
# model with counterfactual_necessity removed
lmer2 <- lmer(data = causalSum,
              causal ~ modalDistance + (1|context))
# model with modalDistance removed
lmer3 <- lmer(data = causalSum,
              causal ~ counterfactual_necessity + (1|context))

causeAnovas <- list(
  # check that interaction between two terms is better than treating them as separate variables
  anova(lmer0, lmer1),
  # check that model without counterfactual_necessity does worse
  anova(lmer1, lmer2),
  # check that model without modal distance does worse
  anova(lmer1, lmer3))

# demonstrate that counterfactual relevance is a modal background representation
relevanceCorr <- with(causalSum, cor.test(modalDistance, counterfactual_relevance))



# Here we compare our model to Icard's model that includes sufficiency judgments

# model with both predictors
lmer0 <- lmer(data = causalSum, causal  ~ necessity_strength + icard_model + (1|context))

# model with our predictor removed
lmer1 <- lmer(data = causalSum, causal  ~  icard_model + (1|context))	

# model with the alternative predictor removed 
lmer2 <- lmer(data = causalSum, causal  ~ necessity_strength + (1|context))	

# model including components of both our and Icard's models
lmer3 <- lmer(data = causalSum, causal  ~ counterfactual_necessity + modalDistance + suffCF + (1|context))	

# model with sufficiency removed
lmer4 <- lmer(data = causalSum, causal  ~ counterfactual_necessity + modalDistance + (1|context))	

suffAnovas <- list(
  # check that our necessity strength predictor is significant
  anova(lmer0,lmer1),
  # check whether the Icard measure is significant
  anova(lmer0,lmer2), 
  # check whether sufficiency adds anything when modal distance and counterfactual_necessity are included
  anova(lmer3, lmer4))	

```
Participants reported a wide range of agreement with statements attributing causal responsibility to the agents ($M=$ `r causalL %>% summarise(mean(response)) %>% pull() %>% round(1)`, $SD=$ `r causalL %>% summarise(sd(response)) %>% pull() %>% round(1)`).\ 
To predict participants' judgments of causal atrtribution for each event, we generated a new measure that incorporated the modal distance for each event and participants' judgments of the counterfactual relevance. The modal distance should only play a role in causal judgments if participants judge the event as necessary for bringing out the downstream consequence. As such, we multiplied participant ratings for counterfactual necessity (data from [Study 5b](#study-5b:-counterfactual-judgments) with modal distance values generating a value which we're calling 'necessity strength' for each event. As predicted, across action-context pairs, necessity strength was highly correlated with causal ratings ($r =$ `r round(potencyCorr$estimate[[1]], 3)`, $p <$ `r max(.001, round(potencyCorr$p.value, 3))`).\ 

The moral distance had a correlation of  $r =$ `r round(causalCorr[[1]]$estimate[[1]], 3)`, $p <$ `r max(.001, round(causalCorr[[1]]$p.value[[1]], 3))` with force judgments; probability distance had a correlation of  $r =$ `r round(causalCorr[[2]]$estimate[[1]], 3)`, $p <$ `r max(.001, round(causalCorr[[2]]$p.value[[1]], 3))` and normality distance had a correlation of  $r =$ `r round(causalCorr[[3]]$estimate[[1]], 3)`, $p <$ `r max(.001, round(causalCorr[[3]]$p.value[[1]], 3))`.\ 

To ensure that both the modal distance and the necessity rating, as well as the interaction between them, were critical for predicting causal ratings, we next conducted a series of model comparisons. Specifically, we built a linear mixed-effects model predicting causal ratings as a function of modal distance, counterfactual ratings and their interaction, with a random effect for context. The full model performed significantly better than a model in which contextual modal distance was removed ($\chi^2$(`r causeAnovas[[3]]$Df[[2]]`) = `r round(causeAnovas[[3]]$Chisq[[2]], 3)`, $p <$ `r max(.001, round( causeAnovas[[3]]$"Pr(>Chisq)"[[2]], 3))`), better than a model in which counterfactual necessity ratings were removed ($\chi^2$(`r causeAnovas[[2]]$Df[[2]]`) = `r round(causeAnovas[[2]]$Chisq[[2]],3)`, $p <$ `r max(.001, round( causeAnovas[[2]]$"Pr(>Chisq)"[[2]], 3))`), and critically, also better than a model in which the interaction between the two was removed ($\chi^2$(`r causeAnovas[[1]]$Df[[2]]`) = `r round(causeAnovas[[1]]$Chisq[[2]],3)`, $p <$ `r max(.001, round(causeAnovas[[1]]$"Pr(>Chisq)"[[2]], 3))`). These model comparisons confirm the importance of the relationship between modal distance and necessity: agents are most strongly judged to be causes of outcomes when it is both the case that there were relevant alternative actions that they could have done instead, and if they had done those actions instead, the outcome would likely have been different. \ 

We additionally found evidence that the counterfactual relevance measure used in prior work reflects modal distance: The contextual modal distance of each actual action was highly correlated with participants' explicit judgments of the relevance of counterfactual alternatives, $r =$ `r round(relevanceCorr$estimate, 3)`, $p <$ `r max(.001, round(relevanceCorr$p.value, 3))`.\	

We also constructed a series of linear mixed effects models to compare our model against the existing model of causal judgments. This model includes a term for sufficiency strength, which we calculated by multiplying the probability of sampling an action by participants' judgments of the sufficiency of the action for causing the downstrema consequence. First we constructed a linear mixed-effects model predicting causal judgments using necessity strength (our measure) and the Icard model's measure. The model performed significantly worse with our measure removed ($\chi^2$(`r suffAnovas[[1]]$Df[[2]]`) = `r round(suffAnovas[[1]]$Chisq[[2]],3)`, $p <$ `r max(.001, round(suffAnovas[[1]]$"Pr(>Chisq)"[[2]], 3))`) but not when the Icard measure was removed ($\chi^2$(`r suffAnovas[[2]]$Df[[2]]`) = `r round(suffAnovas[[2]]$Chisq[[2]],3)`, $p =$ `r max(.001, round(suffAnovas[[2]]$"Pr(>Chisq)"[[2]], 3))`). Because the primary difference between the two measures is that the Icard model includes a term for sufficiency judgments, we constructed another model to see whether sufficiency judgments of the actions contributed anything to causal judgments. We constructed a mixed-effects model predicting causal judgments with necessity strength and sufficiency and a random effect for context. Removing the sufficiency ratings did not significantly effect performance of the model ($\chi^2$(`r suffAnovas[[3]]$Df[[2]]`) = `r round(suffAnovas[[3]]$Chisq[[2]],3)`, $p =$ `r max(.001, round(suffAnovas[[3]]$"Pr(>Chisq)"[[2]], 3))`). \ 


```{r causalFigs, echo=F,warning=F,message=F}
## Figure 6
# Necessity strength vs causal judgments
fig6 <- causalSum %>% mutate(context=factor(context)) %>%
                  ggplot(aes(y=causal,x=necessity_strength)) +
                  geom_point(aes(color=context)) +
                  labs(y="Agreement that the agent should be blamed", 
                       x="Necessity strength of the actual action") +
                  geom_smooth(method=lm)+
                  theme_bw() +
                  theme(
                    plot.background = element_blank()
                    #,legend.position = "top"
                    ,panel.grid.major = element_blank()
                    ,panel.grid.minor = element_blank()
                    ,axis.text=element_text(size=rel(1.25))
                    #,axis.title=element_blank()
                    #,axis.title.y=element_blank()
                    #,axis.text.x=element_text(size=rel(1.5))
                    ,axis.title = element_text(size=rel(1.25))
                    ,axis.ticks = element_blank()
                  )
fig6
# ggsave(fig6,file="../figs/fig6.png", width = 10, height = 8)

## Figure S3
# Counterfactual relevance vs modal distance for each action
figS3 <- causalSum %>% mutate(context=factor(context)) %>%
                  ggplot(aes(y=modalDistance,x=counterfactual_relevance)) +
                  geom_point(aes(color=context)) +
                  labs(y="Modal distance of the actual action", 
                       x="Counterfactual relevance score") +
                  geom_smooth(method=lm)+
                  theme_bw() +
                  theme(
                    plot.background = element_blank()
                    #,legend.position = "top"
                    ,panel.grid.major = element_blank()
                    ,panel.grid.minor = element_blank()
                    ,axis.text=element_text(size=rel(1.25))
                    #,axis.title=element_blank()
                    #,axis.title.y=element_blank()
                    #,axis.text.x=element_text(size=rel(1.5))
                    ,axis.title = element_text(size=rel(1.25))
                    ,axis.ticks = element_blank()
                  )


# ggsave(figS3,file="../figs/figS3.png", width = 10, height = 8)


g <- ggplot_build(figS3)
colors <- unique(g$data[[1]]$colour)
colors <- colors[c(1,13,15)]

```

## Study 6: Blame judgments

Using the same stimulus sets as [Study 5](#study-5), we sought to determine whether the modal distance could further be used to predict participants' moral judgments of the agents. Specifically, we investigated whether participants judged that the agent should be blamed for the downstream consequence occurring.

### Participants

```{r participantsblame, echo=F, warning=F,message=F}
# reads in data for whether ratings of agent should be blamed for downstream consequence
blameW <- read.csv("../data/blame.csv") %>% 
  rownames_to_column("id")
```

We collected a sample of `r length(unique(blameW$id))` participants ($M_{age}$ = `r round(mean(blameW$age,na.rm=T),2)`; $SD_{age}$ = `r round(sd(blameW$age,na.rm=T),2)`; `r table(blameW$gender)[[2]]` females) from Amazon Mechanical Turk ([www.mturk.com](www.mturk.com)).

### Procedure

Study design was identical to [Study 5a](#study-5a), except that instead of being asked about causal attribution, they were instead asked to rate their agreement with a statement of blame attribution on a 100-point scale:

|     *Blame statement:* [Agent] should be blamed for [Downstream consequence].

### Results

```{r tidyBlame, echo=F,warning=F,message=F}
# pivots table to long format
blameL <- blameW %>% 
  filter(Progress ==100) %>% 
  select(id, S1_1:S18_1) %>% 
  pivot_longer(-id, names_to = "context", values_to = "response",
               values_drop_na = T) %>% 
  mutate(across(1:2, parse_number))

# joins with which actions each participant viewed for each context
blameL <- blameW  %>%
  filter(Progress ==100) %>% 
  select(id, S1A:S18A) %>% 
  pivot_longer(-id, names_to = "context", values_to = "action") %>% 
  filter(action != "") %>% 
  mutate(across(1:2, parse_number)) %>% 
  right_join(blameL, by = c("id", "context"))

# creates summary table with average blame ratings for each action
blameSum <- blameL %>% 
  group_by(context, action) %>% 
  summarise(blame = mean(response), .groups = "drop") %>% 
  group_by(context) %>% 
  mutate(actionNumb = paste(context, row_number(), sep = "_"), .after = context)

# adds modal distance values
blameSum <- blameSum %>%
  full_join(causalSum %>% select(context, actionNumb, counterfactual_necessity, necessity_strength, modalDistance, moralDistance, probDistance, normDistance),
                       by = c("context", "actionNumb"))

blameCorr <- with(blameSum,cor.test(necessity_strength, blame))
blameCorrs <- with(blameSum, list(cor.test(moralDistance, blame),
                     cor.test(probDistance, blame),
                     cor.test(normDistance, blame)))

# full model
lmer0 <- lmer(data = blameSum,
              blame ~ counterfactual_necessity * modalDistance + (1|context))
# model with interaction removed
lmer1 <- lmer(data = blameSum,
     blame ~ counterfactual_necessity + modalDistance + (1|context))
# model with counterfactual_necessity removed
lmer2 <- lmer(data = blameSum,
              blame ~ modalDistance + (1|context))
# model with modalDistance removed
lmer3 <- lmer(data = blameSum,
              blame ~ counterfactual_necessity + (1|context))

blameAnovas <- list(
  # check that interaction between two terms is better than treating them as separate variables
  anova(lmer0, lmer1),
  # check that model without counterfactual_necessity does worse
  anova(lmer1, lmer2),
  # check that model without modal distance does worse
  anova(lmer1, lmer3))

```

Once again, participants reported a wide range of agreement with statements attributing blame to the agent ($M=$ `r blameL %>% summarise(mean(response)) %>% pull() %>% round(1)`, $SD=$ `r blameL %>% summarise(sd(response)) %>% pull() %>% round(1)`).	\ 

As with causal judgments, necessity strength was highly correlated with attributions of blame to the agent who acted ($r =$ `r round(blameCorr$estimate[[1]], 3)`, $p <$ `r max(.001, round(potencyCorr$p.value, 3))`). Additionally, we used the same linear mixed effects model we previously used to predict causal judgments to predict attribution of blame. We again selectively lesioned the full model to create three new models: one with modal distance removed, one with counterfactual necessity removed, and one with the interaction term between the two removed. All three models performed significantly worse than the full model (modal distance removed: $\chi^2$(`r blameAnovas[[3]]$Df[[2]]`) = `r round(blameAnovas[[3]]$Chisq[[2]], 3)`, $p <$ `r max(.001, round( blameAnovas[[3]]$"Pr(>Chisq)"[[2]], 3))`; counterfactual necessity removed: $\chi^2$(`r blameAnovas[[2]]$Df[[2]]`) = `r round(blameAnovas[[2]]$Chisq[[2]], 3)`, $p <$ `r max(.001, round( blameAnovas[[2]]$"Pr(>Chisq)"[[2]], 3))`; interaction removed: $\chi^2$(`r blameAnovas[[1]]$Df[[2]]`) = `r round(blameAnovas[[1]]$Chisq[[2]], 3)`, $p <$ `r max(.001, round(blameAnovas[[1]]$"Pr(>Chisq)"[[2]], 3))`). These model comparisons again confirm the importance of the relationship between modal distance and necessity: agents are most held responsible when it is both the case that there were better options available and if they had chosen those alternative options instead, the outcome would have likely been different. \ 

The moral distance had a correlation of  $r =$ `r round(blameCorrs[[1]]$estimate[[1]], 3)`, $p <$ `r max(.001, round(blameCorrs[[1]]$p.value[[1]], 3))` with moral attribution judgments; probability distance had a correlation of  $r =$ `r round(blameCorrs[[2]]$estimate[[1]], 3)`, $p <$ `r max(.001, round(blameCorrs[[2]]$p.value[[1]], 3))` and normality distance had a correlation of  $r =$ `r round(blameCorrs[[3]]$estimate[[1]], 3)`, $p <$ `r max(.001, round(blameCorrs[[3]]$p.value[[1]], 3))`.

```{r figS4, echo=F,warning=F,message=F}
## Figure S4 

# necessity strength vs blame judgments for each action
figS4 <- blameSum %>% mutate(context=factor(context)) %>%
                  ggplot(aes(y=blame,x=necessity_strength)) +
                  geom_point(aes(color=context)) +
                  labs(y="Agreement that the agent should be blamed", 
                       x="Necessity strength of the actual action") +
                  geom_smooth(method=lm)+
                  theme_bw() +
                  theme(
                    plot.background = element_blank()
                    #,legend.position = "top"
                    ,panel.grid.major = element_blank()
                    ,panel.grid.minor = element_blank()
                    ,axis.text=element_text(size=rel(1.25))
                    #,axis.title=element_blank()
                    #,axis.title.y=element_blank()
                    #,axis.text.x=element_text(size=rel(1.5))
                    ,axis.title = element_text(size=rel(1.25))
                    ,axis.ticks = element_blank()
                  )
figS4
# ggsave(figS4, file="../figs/figS4.png",width=10, height=8, units="in")

```


## Appendix

### Table 1

```{r contextsTable, results='asis'}
table1 <- read.csv("../materials/contextsTable.csv") 

# table1 <- table1 %>% rename_with(~sub("(.)", "\\U\\1",., perl=TRUE) %>% gsub('_', ' ',.))

kable(table1,booktabs=T)
```

### Table 2

```{r codingKey, results='asis'}
kable(codingKey)
```

<!-- ### Table 3 -->

<!-- ```{r altcontextsTable, results='asis'} -->
<!-- table3 <- read.csv("../materials/contextsTable.csv") %>% select(context, text, alt_text) -->

<!-- kable(table3,booktabs=T) -->
<!-- ``` -->
